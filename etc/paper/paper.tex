%
% File emnlp2018.tex
%
%% Based on the style files for EMNLP 2018, which were
%% Based on the style files for ACL 2018, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{emnlp2018}
\usepackage{times}
\usepackage{latexsym}
\usepackage{multirow}
\usepackage{url}

\aclfinalcopy % Uncomment this line for the final submission

\setlength\titlebox{8cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B{\sc ib}\TeX}
\newcommand\confname{EMNLP 2018}
\newcommand\conforg{SIGDAT}

% Lingfei's packages from nips17
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{amssymb,amsmath,amsthm,amsfonts}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{subcaption}
% \usepackage{hyperref}
% % \hypersetup{
% %     colorlinks=true,
% %     linkcolor=blue,
% %     filecolor=magenta,      
% %     urlcolor=cyan,
% % }
\newcommand{\Blue}[1]{{\color{blue}#1}}
\newcommand{\Red}[1]{{\color{red}#1}}
\newcommand{\Magenta}[1]{{\color{magenta}#1}}   
\newcommand{\R}{\mathbb{R}}
\newcommand{\W}{\mathcal{W}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\V}{\mathcal{V}}
\newcommand{\1}{\boldsymbol{1}}
\newcommand{\bv}{\boldsymbol{v}}
\newcommand{\bu}{\boldsymbol{u}}
\newcommand{\bz}{\boldsymbol{z}}
\newcommand{\f}{\boldsymbol{f}}
\newcommand{\WMD}{\text{WMD}}
\newcommand{\softmin}{\mathit{softmin}}
\newcommand{\softmax}{\mathit{softmax}}
\newcommand{\T}{\mathsf{T}}
\newcommand{\E}{\mathcal{E}}


\title{Title: \\Subtitle}

\author{
  John Doe \\
  Affiliation\\
  \texttt{johndoe@gmail.com} \\
  \And
  John Doe \\
  Affiliation\\
  \texttt{johndoe@gmail.com} \\
  \And
  John Doe \\
  Affiliation\\
  \texttt{johndoe@gmail.com} \\
}
%\author{First Author \\
%  Affiliation / Address line 1 \\
%  Affiliation / Address line 2 \\
%  Affiliation / Address line 3 \\
%  {\tt email@domain} \\\And
%  Second Author \\
%  Affiliation / Address line 1 \\
%  Affiliation / Address line 2 \\
%  Affiliation / Address line 3 \\
%  {\tt email@domain} \\}

\date{}

\begin{document}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Unknown Token Generation}  

\subsection{Avoiding Error-Prone Methods}

ELMo can be useful for tasks in which it may be necessary to generate word embeddings for unknown words. As it is a purely character based representation, it will always generate a valid word embedding for any sequence of characters. Therefore, it can be useful in tasks where simply ignoring unknown words is not an option, as we may be using models that take word position into account.

Although in some cases unknown words may simply be replaced by a symbol such as a UNKNOWN, it can lead to highly distorted results if positional details about the word embedding for this symbol are taken for granted, as it may happen to be placed inside a densely connected cluster of correlated words within the network. 

Hartmann et. al. \cite{hartmann2017portuguese} achieved distorted benchmarks for the ASSIN sentence similarity task with FastText by inadvertently using the string \textit{unk} as a token for unknown words. Due to the employment of character n-grams, FastText mapped \textit{unk} to a cluster of words commonly associated with musical subjects, such as \textit{g-funk, g-punk and punk-funk}. This has led the authors of the paper to reach benchmarks even 0.06 points above their true mean-squared error value, and therefore reaching the misguided conclusion that FastText embeddings exhibit a high variance in performance between semantic analogies and sentence similarity tasks.

\subsection{Improper Candidate Detection}


So-called meaningless word embeddings that in fact carry high significance can be spotted through visual inspection of their nearest neighbors. However, they can also be automatically detected through graph theoretic methods. 

High clustering coefficient and small characteristic path length are reliable indicators of a small-world subgraph. \cite[p.35]{Cecchini2017GraphbasedCA}. If our unknown token $unk$ belongs to such a graph, it can significantly impact our results, specially if it replaces a large percentage of the tokens.

Let $G$ be the graph formed by taking all words within our model as nodes. There will be an edge between two nodes $a$ and $b$ if their cosine similarity $s(a,b)$ is above a certain threshold $t$, and it will be attributed to this edge a weight of $1 - s(a,b)$.

We wish to know whether a candidate for unknown token $u$ belongs to a small-world subgraph $S'$ within $G$. Therefore, we perform a random walk starting from $u$, proceeding to a neighboring node $c$ on each step, which may or may not already have been visited. If no sudden variations in the local clustering coefficient of $c$ and the characteristic path length of the neighborhood $N_{G}[c]$ occur within the first few steps, then this is a strong indicator that we started our random walk inside a small-world subgraph, and therefore it may be presumed that our unknown token candidate $u$ has the potential of distorting our results.


< UNFINISHED >
\bibliography{references}
\end{document}