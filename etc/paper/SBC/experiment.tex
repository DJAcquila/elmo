\section{Evaluation}

In order to evaluate the robustness of the word embedding models we trained, we performed intrinsic and extrinsic evaluations. For the intrinsic evaluation, we used the set of syntactic and semantic analogies from \cite{rodriguesetal2016}. For extrinsic evaluation, we chose to apply the trained models on POS tagging and sentence similarity tasks. The tasks were chosen deliberately since they are linguistically aligned with the sets of analogies used in the first evaluation. POS tagging is by nature a morphosyntactic task, and although some analogies are traditionally regarded as \emph{syntactic}, they are actually morphological --- for example, suffix operations. Sentence similarity is a semantic task since it evaluates if two sentences have similar meaning. It is expected that the models that achieve the best results in syntactic (morphological) analogies also do so in POS tagging, and the same is true for semantic analogies and semantic similarity evaluation. We trained embeddings with the following dimensions numbers: 50, 100, 300, 600 and 1,000.
 %\todo[inline]{(erick) não entendi pq no parágrafo acima está dito que consideramos POS tagging uma tarefa sintática. Seria para fazer um paralelo com as avaliações intrínsecas sinático/semânticas??? Se for, não tem nada a ver. A avaliação intrínseca chamada carinhosamente de sintática é puramente morfológica (são palavras isoladas!!!); e de qualquer forma, não precisamos nos limitar a fazer paralelos da avaliação intrínseca com a extrínseca.}

%We trained embeddings with the following dimensions numbers: 50, 100, 300, 600 and 1,000. It should be noted that we suffered from RAM restrictions and, for some models, it was not possible to train word embeddings of higher dimensions.


\subsection{Intrinsic evaluation}

We evaluated our embeddings in the syntactic and semantic analogies provided by \cite{rodriguesetal2016}. Since our corpus is composed of both Brazilian (PT-BR) and European (PT-EU) Portuguese, we also evaluated the models in the test sets for both variants, following \cite{rodriguesetal2016}.

Table \ref{tab:evaluation} shows the obtained results for the intrinsic evaluation. On average, GloVe was the best model for both Portuguese variants. The model which best performed on syntactic analogies was FastText, followed by Wang2Vec. This makes sense since FastText is a morphological model, and Wang2Vec uses word order, which provides some minimal syntactic knowledge. In semantic analogies, the model which best performed was GloVe, followed by Wang2Vec. GloVe is known for modeling semantic information well. Wang2Vec potentially captures semantics because it uses word order. The position of a negation in a sentence can totally change its semantics. If this negation is shuffled in a bag of words (Word2Vec CBOW), sentence semantic is diluted.
%\todo[inline]{(nathan) inserir um exemplo de negação em uma sentença para mostrar como a posição dela influencia diretamente na semântica da sentença. No caso do bag-of-words, não se sabe a posição da negação e perdemos poder semântico.}
%\todo[inline]{(erick) o fato de o wang2vec usar word order foi dado como explicação para sua performance nos dois modelos. para o "sintático" até vai, mas para o semântico não faz sentido sem elaborar mais. Chris to Carioca: talvez pega semantica pelo conexto, pode elaborar mais nese sentido pois creio que vai disambiguar a semantica pelas palavras vizinhas, para o argumento sintatico as dependencias podem ser inferidas nas mesmas janelas. Nathan: a ordem é extremamente importante para a semântica. O local onde uma negação ocorre pode alterar completamente a semântica do contexto.}

All CBOW models, except for the Wang2Vec ones, achieved very low results in semantic analogies, similarly to the results from \cite{mikolovetal2013}. %It is interesting because CBOW uses information from the surrounding words in its prediction, and contextual information should be useful in semantics.
%\notaerick{discordo, isso é mais útil para sintaxe. opinao do Chris: da para disambiguar a semantica com contexto como no trabalho do fernando, claro que a sintaxe vai aproveitar ainda mais mas a semantica vai se benificiar bastante. Nathan: a ordem é extremamente importante para a semântica. O local onde uma negação ocorre pode alterar completamente a semântica do contexto.}
%However, Wang2Vec CBOW achieved semantic results comparable to the good ones achieved by other methods. 
Wang2Vec CBOW differs from other CBOW methods in that it takes word order into account, and then we can speculate that an unordered bag-of-words is not able to capture a word's semantic so well.

%\todo[inline]{(erick) Faltou mencionar como isso se compara com a literatura! no trabalho original do mikolov, já havia exatamente isso: o CBOW teve resultados muito ruins em analogias semânticas. Tendo isso em mente, o interessante desses resultados é o wang2vec cbow ter obtido bons resultados. (SANDRA) Erick, escreva !!! O artigo é seu também. Você tem um ótimo inglês :)}
%We also compared our trained models with the best model of \cite{sousa2016}, a 300 dimensions CBOW, in despite of the author just evaluated for Brazilian Portuguese. Table \ref{tab:evaluation} shows obtained results.


\begin{table}[t]
	\center
    \footnotesize
    \scalebox{.73}{
    \begin{tabular}{llrccc|ccc}
    	\toprule
        \multicolumn{2}{c}{\multirow{2}{*}{\textbf{Embedding Models}}} & \multirow{2}{*}{\textbf{Size}} & \multicolumn{3}{c}{\textbf{PT-BR}} & \multicolumn{3}{c}{\textbf{PT-EU}}\\
        \cmidrule{4-9}
        \multicolumn{2}{c}{} & & \textbf{Syntactic} & \textbf{Semantic} & \textbf{All} & \textbf{Syntactic} & \textbf{Semantic} & \textbf{All}\\
        \midrule
        & & 50 	                & 35.2 & 4.2 & 19.6 & 35.2 & 4.6 & 19.8 \\
        %\cmidrule{3-9}
        & & 100                 & 45.0 & 6.1 & 25.5 & 45.1 & 6.4 & 25.7  \\
        %\cmidrule{3-9}
        & CBOW & 300            & 52.0 & 8.4 & 30.1 & 52.0 & 9.1 & 30.5  \\
        %\cmidrule{3-9}
        \multirow{2}{*}{FastText} & & 600                 & 52.6 & 5.9 & 29.2 & 52.4 & 6.5 & 29.4\\
        & & 1,000 & 50.6 & 4.8 & 27.7 & 50.4 & 5.4 & 27.9\\
        \cmidrule{2-9}
        &  & 50                 & 36.8 & 18.4 & 27.6 & 36.5 & 17.1 & 26.8  \\
        %\cmidrule{3-9}
        &  & 100 & 50.8 & 30.0 & 40.4 & 50.7 & 28.9 & 39.8   \\
        %\cmidrule{3-9}
        & Skip-Gram & 300 & \textbf{58.7} & 32.2 & 45.4 & \textbf{58.5} & 31.1 & 44.8  \\
        %\cmidrule{3-9}
        & & 600 & 55.1 & 24.3 & 39.6 & 55.0 & 23.9 & 39.4\\
        & & 1,000 & 45.1 & 14.6 & 29.8 & 45.2 & 13.8 & 29.4\\
        \midrule         
        &  & 50 & 28.7 & 13.7 & 27.4 & 28.5 & 12.8 & 27.7\\
        %\cmidrule{3-9}
         & & 100 & 39.7 & 28.7 & 34.2 & 39.9 & 26.6 & 33.2\\
        %\cmidrule{3-9}
        GloVe & & 300 & 45.8 & 45.8 & \textbf{46.7} & 45.9 & 42.3 & \textbf{46.2}\\
        %\cmidrule{3-9}
        %& & 400 & 43.8 & \textbf{46.9} & 45.4 & 44.2 & \textbf{43.0} & 43.6\\
        & & 600 & 42.3 & \textbf{48.5} & 45.4 & 42.3 & \textbf{43.8} & 43.1\\
        & & 1,000 & 39.4 & 45.9 & 42.7 & 39.8 & 42.5 & 41.1\\
        \midrule
		 & & 50 & 28.4 & 9.2 & 18.8 & 28.4 & 8.9 & 18.6\\
        %\cmidrule{3-9}
        &  & 100 & 40.9 & 26.2 & 33.5 & 40.8 & 24.4 & 32.6\\
        %\cmidrule{3-9}
         & CBOW & 300 & 49.9 & 40.3 & 45.1 & 50.0 & 36.9 & 43.5\\
        & & 600 & 46.1 & 22.2 & 34.1 & 46.0 & 21.1 & 33.5\\
        \multirow{2}{*}{Wang2Vec} & & 1,000 & 44.8 & 21.9 & 33.3 & 44.7 & 20.5 & 32.6\\
        %\cmidrule{3-9}
        %\multirow{2}{*}{Wang2Vec} & & 400 & & & & & &\\
        \cmidrule{2-9}
        &  & 50 & 30.6 & 12.2 & 21.3 & 30.6 & 11.5 & 21.0\\
        %\cmidrule{3-9}
        &  & 100 & 43.9 & 22.2 & 33.0 & 44.0 & 21.2 & 32.6 \\
        %\cmidrule{3-9}
        & Skip-Gram& 300 & 53.3 & 33.9 &  42.8 & 53.4 & 32.3 & 43.6\\
        %\cmidrule{3-9}
        %& & 400 & & & & & &\\
        & & 600 & 52.9 & 35.0 & 43.9 & 53.0 & 33.2 & 43.1\\
        & & 1,000 & 47.3 & 33.2 & 40.2 & 47.6 & 30.9 & 39.2\\
        \midrule
    	% & & 300 \cite{sousa2016} & 21.7 & 17.2 & 20.4 & - & - & -\\
        %\cmidrule{3-9}
        & & 50 & 9.8 & 2.2 & 6.0 & 9.7 & 1.9 & 5.8\\
        %\cmidrule{3-9}
        & & 100 & 16.2 & 3.6 & 9.9 & 16.0 & 3.5 & 9.7\\
        %\cmidrule{3-9}
        & CBOW & 300 & 24.7 & 4.6 & 23.9 & 24.5 & 4.5 & 23.6\\
        %\cmidrule{3-9}
        & & 600 & 25.8 & 5.2 & 23.1 & 25.4 & 5.1 & 22.9\\
        %\cmidrule{3-9}
        \multirow{2}{*}{Word2Vec} & & 1,000 & 26.2 & 4.9 & 22.9 & 26.2 & 4.5 & 22.7\\
        \cmidrule{2-9}
        &  & 50 & 17.0 & 5.4 & 11.2 & 16.9 & 4.8 & 10.8\\
        %\cmidrule{3-9}
        & & 100 & 25.2 & 8.0 & 16.6 & 24.8 & 7.4 & 16.1 \\
        %\cmidrule{3-9}
        & Skip-Gram & 300 & 33.0 & 15.6 & 29.2 & 32.2 & 14.1 & 29.8\\
        %\cmidrule{3-9}
       % & & 600 \cite{hartmann2016} & 33.0 & 17.9 & 25.5 & 32.9 & 15.5 & 24.2\\
        %\cmidrule{3-9}
        & & 600 & 35.6 & 20.0 & 33.4 & 35.3 & 17.6 & 33.5\\
        %\cmidrule{3-9}
        & & 1,000 & 34.1 & 21.3 & 32.6 & 33.6 & 18.1 & 31.9\\
        \bottomrule
    \end{tabular}}
    \caption{Intrinsic evaluation on syntactic and semantic analogies.}
    \label{tab:evaluation}
\end{table}


\subsection{Extrinsic Evaluation}

In this section we describe the experiments performed on POS tagging and Semantic Similarity tasks.

\subsubsection*{POS Tagging}

  POS tagging is a very suitable NLP task to evaluate how well the embeddings capture morphosyntactic properties. The two key difficulties here are: i) correctly classifying words that can have different tags depending on context; and ii) generalizing to previously unseen words.
    Our experiments were performed with the nlpnet POS tagger\footnote{More info at \url{http://nilc.icmc.usp.br/nlpnet/}} using the revised Mac-Morpho corpus and similar tagger configurations to those presented by \cite{Fonseca2015} (20 epochs, 100 hidden neurons, learning rate starting at 0.01, capitalization, suffix and prefix features). We did not focus on optimizing hyperparameters; instead, we set a single configuration to compare embeddings.
    
    Table~\ref{tab:pos} presents the POS accuracy results\footnote{Note that accuracies are well below those reported by \cite{Fonseca2015}. The probable cause is that the embedding vocabularies used here did not have clitic pronouns split from verbs, resulting in a great amount of out of vocabulary words.}.  As a rule of thumb, the larger the dimensionality, the better the performance. The exception is the 1,000 dimensions Word2Vec models, which performed slightly worse than those with 600. GloVe and FastText yielded the worst results, and Wang2Vec achieved the best. GloVe's poor performance may be explained by its focus on semantics rather than syntax, and FastText's performance was surprising in that despite its preference for morphology, something traditionally regarded as important for POS tagging, it yielded relatively poor results. Wang2Vec resulted in the best performance -- actually, its 300 dimension Skip-Gram model was superior to Word2Vec's 1000 model. Concerning the CBOW and Skip-Gram strategies, in the case of FastText, the latter was considerably better. For Wang2Vec and Word2Vec, the gap between the two is less noticeable, where CBOW achieved slightly better performance on smaller dimensionalities. 

\begin{table}[htb]
\centering
\footnotesize
\scalebox{.73}{
\begin{tabular}[t]{@{}llrr@{}}
\toprule
\multicolumn{2}{c}{\textbf{Embedding Models}}          & \multicolumn{1}{c}{\textbf{Size}} & \multicolumn{1}{c}{\textbf{Accuracy}} \\ \midrule
\multirow{10}{*}{FastText} & \multirow{5}{*}{CBOW}      & 50                                & 91.18\%                               \\
                          &                            & 100                               & 92.57\%                               \\
                          &                            & 300                               & 93.86\%                               \\
                          &                            & 600                               & 93.86\%                               \\ 
                          & & 1000 & 94.27\% \\
                          \cmidrule(l){2-4} 
                          & \multirow{5}{*}{Skip-Gram} & 50                                & 93.15\%                               \\
                          &                            & 100                               & 93.78\%                               \\
                          &                            & 300                               & 94.82\%                               \\
                          &                            & 600                               & 95.25\%                               \\
                          & & 1000 & 95.49\% \\ \midrule
 & \multirow{3}{*}{CBOW}      & 50                                & 95.33\%                               \\
                          &                            & 100                               & 95.59\%                               \\
\multirow{2}{*}{Wang2Vec}                          &                            & 300                               & 95.83\%                               \\ \cmidrule(l){2-4} 
                          & \multirow{5}{*}{Skip-Gram} & 50                                & 95.07\%                               \\
                          &                            & 100                               & 95.57\%                               \\
                          &                            & 300                               & 95.89\%                               \\
 &  & 600                               &       95.88\%                         \\
 & & 1,000 & \textbf{95.94\%} \\
\bottomrule
\end{tabular}}
\quad
\scalebox{0.73}{
\begin{tabular}[t]{@{}llrr@{}}
\toprule
\multicolumn{2}{c}{\textbf{Embeddings model}}           & \multicolumn{1}{c}{\textbf{Size}} & \multicolumn{1}{c}{\textbf{Accuracy}} \\ \midrule
\multirow{5}{*}{GloVe}     & \multirow{4}{*}{}         & 50                                & 93.13\%                               \\
                           &                            & 100                               & 93.72\%                               \\
                           &                            & 300                               & 94.76\%                               \\
                           &                            %& 400                               & 94.95\%                               \\ 
			& 600                               & 95.23\%\\
            & & 1,000                               & 95.57\%\\
                           \midrule
\multirow{10}{*}{Word2Vec} & \multirow{5}{*}{CBOW}      & 50                                & 95.00\%                               \\
                           &                            & 100                               & 95.27\%                               \\
                           &                            & 300                               & 95.58\%                               \\
                           &                            & 600                               & 95.65\%                               \\
                           &                            & 1,000                              & 95.62\%                               \\ \cmidrule(l){2-4} 
                           & \multirow{5}{*}{Skip-Gram} & 50                                & 94.79\%                               \\
                           &                            & 100                               & 95.18\%                               \\
                           &                            & 300                               & 95.66\%                               \\
                           &                            & 600                               & 95.82\%                               \\
                           &                            & 1,000                              & 95.81\%                               \\ \bottomrule
\end{tabular}}
\caption{Extrinsic evaluation on POS tagging}
\label{tab:pos}
\end{table}


\subsubsection*{Semantic Similarity}

ASSIN (\textit{Avaliação de Similaridade Semântica e Inferência Textual}) was a workshop co-located with PROPOR-2016. ASSIN made two shared-tasks available: i) semantic similarity; and ii) entailment. We chose the first one to evaluate our word embedding models extrinsically in a semantic task. ASSIN semantic similarity shared task required participants to assign similarity values between 1 and 5 to pairs of sentences. The workshop made training and test sets for Brazilian (PT-BR) and European (PT-EU) Portuguese available. \cite{hartmann2016} obtained the best results for this task. The author calculated the semantic similarity of pairs of sentences training a linear regressor with two features: i) the cosine similarity between the TF-IDF of each sentence; and ii) the cosine similarity between the summation of the word embeddings of the sentences' words. We chose this work as a baseline for evaluation because we can replace its word embedding model with others and compare the results. Although the combination of TF-IDF and word embeddings produced better results than only using word embeddings, we chose to only use embeddings for ease of comparison. \cite{hartmann2016} trained the word embedding model using Word2Vec Skip-Gram approach, with 600 dimensions, and a corpus composed of Wikipedia, G1 and PLN-Br. Only using embeddings, \cite{hartmann2016} achieved 0.58 in Pearson's Correlation ($\rho$) and a 0.50 Mean Squared Error (MSE) for PT-BR; and 0.55 $\rho$ and 0.83 MSE for PT-EU evaluation.

Table \ref{tab:semantic_similarity_evaluation} shows the performance of our word embedding models for both PT-BR and PT-EU test sets. To our surprise, the word embedding models which achieved the best results on semantic analogies (see Table \ref{tab:evaluation}) were not the best in this semantic task. The best results for European Portuguese was achieved by Word2Vec CBOW model using 1,000 dimensions. CBOW models were the worst on semantic analogies and were not expected to achieve the best results in this task. The best result for Brazilian Portuguese was obtained by Wang2Vec Skip-Gram model using 1,000 dimensions. This model also achieved the best results for POS tagging. Neither FastText nor GloVe models beat the results achieved by \cite{hartmann2016}.

% \begin{table}[!ht]
% \center
% \footnotesize
% \scalebox{1}{
% \begin{tabular}{llrrr|rr}
% \toprule
% \multicolumn{2}{c}{\multirow{2}{*}{\textbf{Embeddings Model}}} & \multirow{2}{*}{\textbf{Size}} & \multicolumn{2}{c}{\textbf{PT-BR}} & \multicolumn{2}{c}{\textbf{PT-PT}} \\
% \cmidrule{4-7}
%  \multicolumn{2}{c}{} & & \textbf{CP} & \textbf{EQM} & \textbf{CP} & \textbf{EQM}\\
% \midrule
% &  & 50 & 0.36 & 0.66 & 0.34 & 1.05\\
% %\cmidrule{3-7}
% & \multirow{2}{*}{CBOW} & 100 & 0.37 & 0.66 & 0.36 & 1.04\\
% %\cmidrule{3-7}
% & & 300 & 0.38 & 0.65 & 0.37 & 1.03\\
% %\cmidrule{3-7}
% \multirow{2}{*}{FastText} & & 600 & 0.33 & 0.68 & 0.38 & 1.02\\
% \cmidrule{2-7}
% & & 50 & 0.45 & 0.61 & 0.43 & 0.98\\
% %\cmidrule{3-7}
% & \multirow{2}{*}{Skip-Gram} & 100 & 0.49 & 0.58 & 0.47 & 0.94\\
% %\cmidrule{3-7}
% & & 300 & 0.55 & 0.53 & 0.40 & 1.02\\
% %\cmidrule{3-7}
% & & 600 & 0.40 & 0.64 & 0.40 & 1.01\\
% \midrule
% & & 50 & 0.42 & 0.62 & 0.38 & 1.01\\
% %\cmidrule{3-7}
% \multirow{2}{*}{GloVe} & & 100 & 0.45 & 0.60 & 0.42 & 0.98\\
% %\cmidrule{3-7}
% & & 300 & 0.49 & 0.58 & 0.45 & 0.95\\
% %\cmidrule{3-7}
% & & 400 & 0.50 & 0.57 & 0.46 & 0.95\\
% \midrule
% &  & 50 & 0.53 & 0.55 & 0.51 & 0.89\\
% %\cmidrule{3-7}
% & CBOW & 100 & 0.56 & 0.52 & 0.54 & \textbf{0.85}\\
% %\cmidrule{3-7}
% \multirow{2}{*}{Wang2Vec} & & 300  & 0.53 & 0.55 & 0.51 & 0.89\\
% \cmidrule{2-7}
% & & 50 & 0.51 & 0.56 & 0.47 & 0.92\\
% %\cmidrule{3-7}
% & Skip-Gram & 100 & 0.54 & 0.54 & 0.50 & 0.89\\
% %\cmidrule{3-7}
% & & 300 & \textbf{0.58} & \textbf{0.50} & 0.53 & \textbf{0.85}\\
% \midrule
% &  & 50 & 0.47 & 0.59 & 0.46 & 0.95\\
% %\cmidrule{3-7}
% &  & 100 & 0.50 & 0.57 & 0.49 & 0.91\\
% %\cmidrule{3-7}
% & CBOW & 300 & 0.55 & 0.53 & 0.54 & 0.87\\
% %\cmidrule{3-7}
% & & 600 & 0.57 & 0.51 & \textbf{0.55} & 0.86\\
% %\cmidrule{3-7}
% \multirow{2}{*}{Word2Vec} & & 1000 & \textbf{0.58} & \textbf{0.50} & \textbf{0.55} & 0.86\\
% \cmidrule{2-7}
% & & 50 & 0.46 & 0.60 & 0.43 & 0.97\\
% %\cmidrule{3-7}
% & & 100 & 0.48 & 0.58 & 0.45 & 0.95\\
% %\cmidrule{3-7}
% & \multirow{2}{*}{Skip-Gram} & 300 & 0.52 & 0.56 & 0.48 & 0.93\\
% %\cmidrule{3-7}
% %& & 600 \cite{hartmann2016} & 0.58 & 0.50 & \textbf{0.55} & 0.83\\
% %\cmidrule{3-7}
% & & 600 & 0.53 & 0.54 & 0.50 & 0.92\\
% %\cmidrule{3-7}
% & & 1000 & 0.54 & 0.54 & 0.50 & 0.91\\
% \bottomrule
% \end{tabular}}
% \vspace{0.1cm}
% \caption{ Extrinsic evaluation of word embeddings models on Semantic Similarity task.}
% \label{tab:semantic_similarity_evaluation}
% \end{table}






\begin{table}[htb]
\centering
\footnotesize
\scalebox{0.73}{
\begin{tabular}[t]{llrrr|rr}
\toprule
\multicolumn{2}{c}{\multirow{2}{*}{\textbf{Embedding Models}}} & \multirow{2}{*}{\textbf{Size}} & \multicolumn{2}{c}{\textbf{PT-BR}} & \multicolumn{2}{c}{\textbf{PT-EU}} \\
\cmidrule{4-7}
 \multicolumn{2}{c}{} & & \textbf{$\rho$} & \textbf{MSE} & \textbf{$\rho$} & \textbf{MSE}\\
\midrule

&  & 50 & 0.36 & 0.66 & 0.34 & 1.05\\
%\cmidrule{3-7}
& & 100 & 0.37 & 0.66 & 0.36 & 1.04\\
%\cmidrule{3-7}
& CBOW & 300 & 0.38 & 0.65 & 0.37 & 1.03\\
%\cmidrule{3-7}
\multirow{2}{*}{FastText} & & 600 & 0.33 & 0.68 & 0.38 & 1.02\\
& & 1,000 & 0.39 & 0.64 & 0.41 & 0.99\\
\cmidrule{2-7}
& & 50 & 0.45 & 0.61 & 0.43 & 0.98\\
%\cmidrule{3-7}
& & 100 & 0.49 & 0.58 & 0.47 & 0.94\\
%\cmidrule{3-7}
& Skip-Gram & 300 & 0.55 & 0.53 & 0.40 & 1.02\\
%\cmidrule{3-7}
& & 600 & 0.40 & 0.64 & 0.40 & 1.01\\
& & 1,000 & 0.52 & 0.56 & 0.54 & 0.86\\
\midrule
 &  & 50 & 0.53 & 0.55 & 0.51 & 0.89\\
%\cmidrule{3-7}
& & 100 & 0.56 & 0.52 & 0.54 & 0.85\\
%\cmidrule{3-7}
& CBOW & 300  & 0.53 & 0.55 & 0.51 & 0.89\\
& & 600 & 0.49 & 0.58 & 0.53 & 0.87\\
\multirow{2}{*}{Wang2Vec}& & 1,000 & 0.50 & 0.57 & 0.53 & 0.87\\
\cmidrule{2-7}
& & 50 & 0.51 & 0.56 & 0.47 & 0.92\\
%\cmidrule{3-7}
&  & 100 & 0.54 & 0.54 & 0.50 & 0.89\\
%\cmidrule{3-7}
& Skip-Gram & 300 & 0.58 & 0.50 & 0.53 & 0.85\\
& & 600 & 0.59 & \textbf{0.49} & 0.54 & \textbf{0.83}\\
& & 1,000 & \textbf{0.60} & \textbf{0.49} & 0.54 & 0.85\\



\bottomrule
\end{tabular}}
\quad
\scalebox{0.73}{
\begin{tabular}[t]{llrrr|rr}
\toprule
\multicolumn{2}{c}{\multirow{2}{*}{\textbf{Embedding Models}}} & \multirow{2}{*}{\textbf{Size}} & \multicolumn{2}{c}{\textbf{PT-BR}} & \multicolumn{2}{c}{\textbf{PT-EU}} \\
\cmidrule{4-7}
 \multicolumn{2}{c}{} & & \textbf{$\rho$} & \textbf{MSE} & \textbf{$\rho$} & \textbf{MSE}\\
\midrule
& & 50 & 0.42 & 0.62 & 0.38 & 1.01\\
%\cmidrule{3-7}
& & 100 & 0.45 & 0.60 & 0.42 & 0.98\\
%\cmidrule{3-7}
GloVe & & 300 & 0.49 & 0.58 & 0.45 & 0.95\\
%\cmidrule{3-7}
%& & 400 & 0.50 & 0.57 & 0.46 & 0.95\\
& & 600 & 0.50 & 0.57 & 0.45 & 0.94\\
& & 1,000 & 0.51 & 0.56 & 0.46 & 0.94\\
\midrule
&  & 50 & 0.47 & 0.59 & 0.46 & 0.95\\
%\cmidrule{3-7}
&  & 100 & 0.50 & 0.57 & 0.49 & 0.91\\
%\cmidrule{3-7}
& CBOW & 300 & 0.55 & 0.53 & 0.54 & 0.87\\
%\cmidrule{3-7}
& & 600 & 0.57 & 0.51 & \textbf{0.55} & 0.86\\
%\cmidrule{3-7}
\multirow{2}{*}{Word2Vec} & & 1,000 & 0.58 & 0.50 & \textbf{0.55} & 0.86\\
\cmidrule{2-7}
& & 50 & 0.46 & 0.60 & 0.43 & 0.97\\
%\cmidrule{3-7}
& & 100 & 0.48 & 0.58 & 0.45 & 0.95\\
%\cmidrule{3-7}
& Skip-Gram & 300 & 0.52 & 0.56 & 0.48 & 0.93\\
%\cmidrule{3-7}
%& & 600 \cite{hartmann2016} & 0.58 & 0.50 & \textbf{0.55} & 0.83\\
%\cmidrule{3-7}
& & 600 & 0.53 & 0.54 & 0.50 & 0.92\\
%\cmidrule{3-7}
& & 1,000 & 0.54 & 0.54 & 0.50 & 0.91\\
\bottomrule
\end{tabular}}
\caption{Extrinsic evaluation on Semantic Similarity task.}
\label{tab:semantic_similarity_evaluation}
\end{table}