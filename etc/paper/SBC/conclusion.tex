\section{Conclusions and Future Work}

%In this paper, we collected a large corpus of Portuguese texts in order to train word embedding models using four different techniques. 
In this paper, we presented the word embeddings we trained using four different techniques and their evaluation.
All trained models are available for download, as well as the script used for corpus preprocessing. The results obtained from intrinsic and extrinsic evaluations were not aligned with each other, contrary to the expected. GloVe produced the best results for syntactic and semantic analogies, and the worst, together with FastText, for both POS tagging and sentence similarity. These results are aligned with those from \cite{repeval:16}, which suggest that word analogies are not appropriate for evaluating word embeddings. Overall, Wang2Vec vectors yielded very good performance across our evaluations, suggesting they can be useful for a variety of NLP tasks.
% A sentença abaixo ocupava 6 linhas e estava cheia de achismos.... sou contra incluí-la no texto final
%As future work, we intend to try different tokenization and normalization patterns, e.g., the one used in the project Universal Dependencies, to evaluate how much tokenization affects the word embedding representativeness in different tasks or better normalization strategies for numbers which should be verbalized non-numerically as well as internet language since these patterns can change semantic and syntactic relationships when properly identified. 
As future work, we intend to try different tokenization and normalization patterns, and also to lemmatize certain word categories like verbs, since this could significantly reduce vocabulary, allowing for more efficient processing. An evaluation with more NLP tasks would also be beneficial to our understanding of different model performances.

%\todo[inline]{(erick) até concordo que tokenização pode dar algumas diferenças interessantes, mas a ponto de ser mencionado nas conclusões em lugar de outras coisas mais interessantes?? (SANDRA) Erick: elenque os trabalhos fututos interessantes! O artigo é seu também :). Chris: eu acho que alem de tokenizacao, lemmatizacao e normalizacao sao muito importantes, os verbos se exandem muito no vocabulario e a gente perde toda semantica onde tem datas, horarios, medidas, abreviacoes, etc. Nathan: lematizar vai perder a riqueza dos verbos mas normalizar datas, horários, dinheiro podem melhorar a semântica das embeddings. Hoje apenas normalizamos números.}

%Interestingly, if Pinocchio said ``my nose will grow now'', this would cause a paradox. lol fun fan fun
