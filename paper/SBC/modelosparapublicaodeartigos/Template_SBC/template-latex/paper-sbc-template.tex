\documentclass[12pt]{article}

\usepackage{sbc-template}
\usepackage{graphicx,url}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}  

     
\sloppy

\title{ELMo Evaluation\\ Subtitle}

\author{Autor 1\inst{1}, Autor 2\inst{2} }


\address{Instituto de Informática -- Universidade Federal de Goiás
  (UFG)\\
\nextinstitute
  Instituto 2
}

\begin{document} 

\maketitle

\begin{abstract}
  Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Dolor sed viverra ipsum nunc aliquet bibendum enim. In massa tempor nec feugiat. Nunc aliquet bibendum enim facilisis gravida. Nisl nunc mi ipsum faucibus vitae aliquet nec ullamcorper. Amet luctus venenatis lectus magna fringilla. 
\end{abstract}
     
\begin{resumo} 
  Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Dolor sed viverra ipsum nunc aliquet bibendum enim. In massa tempor nec feugiat. Nunc aliquet bibendum enim facilisis gravida. Nisl nunc mi ipsum faucibus vitae aliquet nec ullamcorper. Amet luctus venenatis lectus magna fringilla. 
\end{resumo}

\section{Semantic Similarity}


For this task we compared the performance of the ELMo embeddings trained by \cite{castro} with the word embeddings trained by \cite{hartmann2017portuguese} under the ASSIN dataset. 

\subsubsection{Smooth Inverse Frequency}

The ASSIN dataset was inspired on the SICK dataset \cite{assin}, and both are targeted towards textual entailment and semantic similarity tasks. However, while the SICK dataset features mostly common words, the ASSIN dataset contains a considerable amount of highly domain-specific terms, as its main sources are Portuguese news websites visited during the year of 2015.

This has an significant impact on 

For some tasks, acceptable results can be obtained by simply ignoring OOV words. However, this may not be a suitable choice, specially if OOV words constitute a significant portion of the dataset, or if factors such as word position are relevant to the task at hand.

It may seem tempting to use a single generic OOV (out of vocabulary) embedding for all words not seen during the training phase, but this approach can lead to highly undesirable results. \cite{} has noticed that such a method fails to tell known words that have been deliberately obfuscated from non-obfuscated and rare words. \cite{hartmann2017portuguese} achieved distorted benchmarks for the ASSIN sentence similarity task with FastText by inadvertently using the string \textit{unk} as a token for unknown words. Due to the employment of character n-grams, FastText mapped \textit{unk} to a cluster of words commonly associated with the subject of pop music, such as \textit{g-funk, g-punk and punk-funk}. This has led the authors of the paper to reach benchmarks highly divergent from what could be obtained by simply ignoring unknown tokens.

Table 1 shows the results reported by \cite{hartmann2017portuguese} side by side with our results on the embeddings that were most strongly affected by using the \textit{unk} token. We repeated their experiments exactly as described, except for the removal of the \textit{unk} tokens from the pre-processed data. Although we reproduced all their sentence similarity tests with exactly the same pre-processed data and word embeddings, other word embeddings did not display such a large difference in test results after the word \textit{unk} was removed.

\subsection{Combined ELMo Embeddings}

An alternative approach 


\cite{gehrmann2019improving} has shown that 

\subsection{Smooth Inverse Frequency}


\begin{table}[]
	\centering
	\caption{Mean squared error and Pearson correlation coefficient }
		\begin{tabular}{lllll}
		& MSE & Pearson & MSE & Pearson \\
		FastText, CBOW (600)       & 0.68              & 0.33                  & 0.63                 & 0.4                      \\
		FastText, skip-gram (100)  & 0.58              & 0.49                  & 0.52                 & 0.55                     \\
		FastText, skip-gram (1000) & 0.56              & 0.52                  & 0.49                 & 0.59                     \\
		FastText, skip-gram (300)  & 0.53              & 0.55                  & 0.5                  & 0.58                     \\
		FastText, skip-gram (50)   & 0.61              & 0.45                  & 0.55                 & 0.52                     \\
		FastText, skip-gram (600)  & 0.64              & 0.40                  & 0.49                 & 0.59                     \\
		Wang2Vec, CBOW (300)       & 0.55              & 0.53                  & 0.5                  & 0.57                    
	\end{tabular}
\end{table}


\renewcommand\refname{References}
\bibliographystyle{sbc}
\bibliography{sbc-template}

\end{document}
