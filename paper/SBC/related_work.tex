\section{Related Work}
  
The research on evaluating unsupervised word embeddings can be divided into intrinsic and extrinsic evaluations. The former relying mostly on word analogies (e.g. \cite{mikolovetal2013}) and measuring the semantic similarity between words (e.g. the WS-353 dataset \cite{finkelstein2001}), while extrinsic evaluations focus on practical NLP tasks  (e.g. \cite{2016nayak-veceval}). POS tagging, parsing, semantic similarity between sentences, and sentiment analysis are some commonly used tasks for this end.

%\todo[inline]{(erick) não entendi o que isso quer dizer... e acho que podemos pular esse trecho de qualquer forma: Intrinsic evaluations measure the quality of word embeddings usually through datasets of query terms; however, they have limitations since the datasets are precompiled and mostly limited to local metrics, such as relatedness \cite{conf/emnlp/SchnabelLMJ15}. ERICK e todos: vale a pena ler o artigo EVATUATION METHODS FOR UNSUPERVISED WORD EMBEDDINGS. Lá eles explicam os 4 tipos de avaliação intrínseca da relatedness: relatedness (correlação de com escores de humanos), analogy (popularizado pelo Mikolov), categorization (clusterização de palavras de categorias diferentes), selecional preference (quão tipico um substantivo está para um verbo ou para sujeito ou para um objeto). DAI o artigo apresenta os vários datasets típicos para o inglês: WordSim 353, MEN. E faz críticas sobre como são construídos esses datasets. Na verdade, nada foi comentado no nosso artigo sobre o dataset que [Rodrigues, 2016] usou. O que acham de explicar ???w}

%On the other hand, extrinsic evaluations seem to be the better option for novel word embeddings methods, although the problem here is to find a representative set of NLP tasks to include in a benchmark suite. \cite{2016nayak-veceval}, for example, propose tasks to test syntactic and semantic properties of the word embeddings, including POS tagging, chunking, named entity recognition, sentiment classification, question classification and phrase-level natural language inference in their suite of relevant downstream tasks.

%\todo[inline]{Aqui já tem que dar uma justificação do porque escolhemos as 2 tarefas}

%\todo[inline]{(Erick) Uma coisa que ficou faltando mencionar é que um tipo comum de avaliação em inglês são os testes de similaridade entre duas palavras, mencionados pelo Faruqui 2016.}

% To the best of our knowledge, only a few works attempted to evaluate Portuguese word embeddings.
% \cite{rodriguesetal2016} collected a corpus of Portuguese texts to train word embedding models using the Skip-Gram Word2Vec technique. Their corpus contains 1,723,693,241 tokens following the Universal Dependencies tokenization pattern. The authors also translated the benchmark of syntactic and semantic analogies developed by \cite{mikolovetal2013} and made it available\footnote{\url{https://github.com/nlx-group/lx-dsemvectors}} for both Brazilian and European Portuguese. They report a 52.8\% evaluation accuracy of their word embedding model in both syntactic and semantic analogies.

To the best of our knowledge, only a few works attempted to evaluate Portuguese word embeddings.
\cite{rodriguesetal2016} collected a corpus of Portuguese texts to train word embedding models using the Skip-Gram Word2Vec technique. The authors also translated the benchmark of word analogies developed by \cite{mikolovetal2013} and made it available\footnote{\url{https://github.com/nlx-group/lx-dsemvectors}} for both Brazilian and European Portuguese. The benchmark contains five types of semantic analogy: (i) common capitals and countries, (ii) all capitals and countries, (iii) currency and countries, (iv) cities and states, and (v) family relations. Moreover, nine types of syntactic analogy are also represented: adjectives and adverbs, opposite adjectives, base adjectives and comparatives, base adjectives and superlatives, verb infinitives and present participles, countries and nationalities (adjectives), verb infinitives and past tense forms, nouns in plural and singular, and verbs in plural and singular. They report a 52.8\% evaluation accuracy of their word embedding model in both syntactic and semantic analogies.

%\todo[inline]{Aqui já tem que mostrar os resultados comparativos com o nosso córpus}

\cite{sousa2016} investigated whether Word2Vec (CBOW and Skip-Gram) or GloVe performed best on the benchmark in \cite{rodriguesetal2016}. The author compiled a sample of texts from Wikipedia in Portuguese, searching for articles related to teaching, education, academics, and institutions. The best results were obtained using Word2Vec CBOW to train vectors of 300 dimensions. This model achieved an accuracy of 21.7\% on syntactic analogies, 17.2\% on semantic analogies and 20.4\% overall.

\cite{Fonseca2015} compared the performance of three different vector space models used for POS tagging with a neural tagger. They used Word2Vec Skip-Gram, HAL, and the neural method from \cite{collobertetal2011}; Skip-Gram obtained the best results in all tests.

Concerning the differences between embeddings obtained from Brazilian and European Portuguese texts, \cite{Fonseca2016} present an extrinsic analysis on POS tagging. They trained different embedding models; one with only Brazilian texts, one with only European ones and another with mixed variants; and trained neural POS taggers which were evaluated on Brazilian and European datasets. One of their findings is that, as a rule of thumb, the bigger the corpus in which embeddings are obtained, the better. Additionally, mixing both variants in the embedding generation did not decrease tagger performance in any of the POS test sets. This supports the hypothesis that a single, large corpus comprising Brazilian and European texts can be useful for most NLP applications in Portuguese.