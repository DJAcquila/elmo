\section{Introduction}

% Natural Language Processing (NLP) applications usually receive texts as an input, for this reason, words can be considered as basic processing units. Therefore, it is important that they are represented in some way that carries relevant information.
% As long more sophisticated NLP and machine learning techniques were developed, more effective representations of words appeared. The work of \cite{bengio2003neural} was one of the pioneers in employing neural networks to automatically learn vector representations. This type of representation is known as word embeddings. Word embeddings are able to capture semantic, syntactic and morphological information from large non-annotated corpus \cite{collobertetal2011,mikolovetal2013, Ling:2015:naacl, lai2015recurrent}.

Natural Language Processing (NLP) applications usually take words as basic input units; therefore, it is important that they be represented in a meaningful way.
In recent years, \emph{word embeddings} have been found to efficiently provide such representations, and consequently, have become common in modern NLP systems. They are vectors of real valued numbers, which represent words in an $n$-dimensional space, learned from large non-annotated corpora and able to capture syntactic, semantic and morphological knowledge.


%O uso de word embeddings é uma abordagem recente que vem obtendo sucesso, as quais possuem uma representação baseada em vetores reais distribuídos em um espaço multidimensional induzidos através de aprendizado não-supervisionado (TURIAN; RATINOV; BENGIO, 2010). Cada dimensão da embedding de uma palavra representa uma feature latente, de modo a capturarem distribuidamente propriedades semânticas, sintáticas ou morfológicas dessa palavra (COLLOBERT et al., 2011). O número de dimensões dos vetores pode variar, e em geral, melhores representações podem ser alcançadas com um maior número de dimensões. Mas se esse número for muito grande, o processamento de indução pode ser demorado.

%In order to apply neural networks in NLP tasks, it is necessary to map words of a text to a numeric vector or word embeddings. This representation allows to capture latent lexical and semantic relations \cite {collobertetal2011, mikolovetal2013}.

Different algorithms have been developed to generate embeddings \cite[\emph{inter alia}]{bengio2003neural, collobertetal2011,mikolovetal2013, Ling:2015:naacl, lai2015recurrent}. They can be roughly divided into two families of methods \cite{baronietal2014}: the first is composed of methods that work with a co-occurrence word matrix, such as Latent Semantic Analysis (LSA) \cite{dumais1988using}, Hyperspace Analogue to Language (HAL) \cite{lund1996producing} and Global Vectors (GloVe) \cite{penningtonetal2014}. The second is composed of predictive methods, which try to predict neighboring words given one or more context words, such as Word2Vec \cite{mikolovetal2013}.

%Word embeddings are being applied to many syntactic and semantic tasks such speech recognition \cite{mikolov2009neural}, semantic similarity \cite{mikolovetal2013}, part-of-speech (POS) tagging, sentiment analysis \cite{li2015multi} and logical semantics \cite{bowman2014recursive}. 

% \cite{mikolovetal2013} developed a benchmark of word embeddings evaluation. This benchmark is composed of instances of analogies which each instance contains 4 words. For example, Berlin, Germany, Lisbon, and Portugal what means ``Berlins is to Germany like Lisbon is to Portugal''. \cite{rodriguesetal2016} translated this benchmark to Portuguese and made it available\footnote{https://github.com/nlx-group/lx-dsemvectors}. This benchmark contains syntactic and semantic analogies. Syntactic analogies consist of adjectives to adverbs, antonyms, comparatives, superlatives, present participle, nationality adjectives, conditional verbs, nouns plural and verbs plural. Semantic analogies consist of common countries and their capitals, all countries and their capitals, currency, cities and states and family relationships.

%\todo[inline]{Primeira vez que fala sobre word embeddings e já entra comentando sobre benchmark de avaliação.}

%\todo[inline]{(Erick) Melhor assim?}

Given this variety of word embedding models, methods for evaluating them becomes a topic of interest. \cite{mikolovetal2013} developed a benchmark for embedding evaluation based on a series of analogies. Each analogy is composed of two pairs of words that share some syntactic or semantic relationship, e.g., the names of two countries and their respective capitals, or two verbs in their present and past tense forms. In order to evaluate an embedding model, applying some vectorial algebra operation to the vectors of three of the words should yield the vector of the fourth one. A version of this dataset translated and adapted to Portuguese was created by \cite{rodriguesetal2016}. %The benchmark contains five types of semantic analogy: (i) common capitals and countries, (ii) all capitals and countries, (iii) currency and countries, (iv) cities and states, and (v) family relations. Moreover, nine types of syntactic analogy are also represented: adjectives and adverbs, opposite adjectives, base adjectives and comparatives, base adjectives and superlatives, verb infinitives and present participles, countries and nationalities (adjectives), verb infinitives and past tense forms, nouns in plural and singular, and verbs in plural and singular. 
%The test set contains a total of 8869 semantic and 10675 syntactic entries.

However, in spite of being popular and computationally cheap, \cite{repeval:16} suggests that word analogies are not appropriate for evaluating embeddings. Instead, they suggest using task-specific evaluations, i.e., to compare word embedding models on how well they perform on downstream NLP tasks.

In this paper, we evaluated different word embedding models trained on a large Portuguese corpus, including both Brazilian and European variants (Section 2). %We used a different tokenization process than \cite{rodriguesetal2016}, and 
We trained our models using four different algorithms with varying dimensions (Section 3). We evaluated them on the aforementioned analogies as well as on POS tagging and sentence similarity, to assess both syntactic and semantic properties of the word embeddings (Section 4). Section 5 revises recent studies evaluating Portuguese word embeddings and compares literature results with ours. The contributions of this paper are: i) to make a set of 31 word embedding models publicly available\footnote{Available at \url{http://nilc.icmc.usp.br/embeddings}} as well as the script used for corpus preprocessing; and ii) an intrinsic and extrinsic evaluation of word embedding models, indicating the lack of correlation between performance in syntactic and semantic analogies and syntactic and semantic NLP tasks.

%In this paper, we focus is the different models of word embeddings in a large corpus of the Portuguese language in both Brazilian and Portuguese variants and their evaluation on syntactic and semantic analogies and syntactic and semantic tasks -- pos tagging and sentence similarity. Our contributions are: 1) make available a set of 32-word embeddings' models; 2) an intrinsic and an extrinsic evaluation of our models; 3) indications of non-correlation between the performance of a word embedding' model in syntactic and semantic analogies and syntactic and semantic tasks.
