\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{zhang2018sentence,gui2016representative,gui2014estimate}
\citation{gong2017prepositions,gong2018embedding}
\citation{mikolov2013exploiting}
\citation{pham2015learning}
\citation{peng2016recurrent,peng2015piefa}
\citation{salton1988term}
\citation{robertson1994some}
\citation{wang2012baselines}
\citation{deerwester1990indexing}
\citation{blei2003latent}
\citation{wu2015preconditioned}
\citation{bengio2003neural,mikolov2013efficient,mikolov2013distributed,pennington2014glove}
\citation{le2014distributed,Chen2017efficient}
\citation{kusner2015word}
\citation{wu2018d2ke}
\citation{haasdonk2004learning}
\citation{wu2018random}
\citation{wu2018d2ke}
\citation{pennington2014glove,wieting2015ppdb}
\citation{huang2016supervised}
\citation{mikolov2013efficient,mikolov2013distributed}
\citation{pennington2014glove,wieting2015ppdb}
\newlabel{sec:Word2Vec and WMD}{{2}{2}{Word2Vec and Word Mover's Distance}{section.2}{}}
\citation{kusner2015word}
\citation{rubner2000earth}
\citation{hitchcock1941distribution,altschuler2017near}
\citation{rubner2000earth}
\citation{kusner2015word}
\citation{wu2018d2ke}
\citation{rahimi2007random}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:wmd}{{1a}{3}{WMD\relax }{figure.caption.1}{}}
\newlabel{sub@fig:wmd}{{a}{3}{WMD\relax }{figure.caption.1}{}}
\newlabel{fig:wme}{{1b}{3}{WME\relax }{figure.caption.1}{}}
\newlabel{sub@fig:wme}{{b}{3}{WME\relax }{figure.caption.1}{}}
\newlabel{fig:wmd_wme_demo}{{1}{3}{An illustration of the WMD and WME. All non-stop words are marked as bold face. WMD measures the distance between two documents. WME approximates a kernel derived from WMD with a set of random documents.\relax }{figure.caption.1}{}}
\newlabel{WMD}{{1}{3}{Word2Vec and Word Mover's Distance}{equation.2.1}{}}
\newlabel{WMK}{{2}{3}{Word Mover's Kernel}{equation.3.2}{}}
\newlabel{D2K2}{{3.1}{3}{Word Mover's Kernel}{equation.3.2}{}}
\citation{arora2016latent,arora2017simple}
\citation{bourgeois1971extension}
\newlabel{softmin}{{3.1}{4}{Word Mover's Kernel}{equation.3.2}{}}
\newlabel{tri_ineq}{{3.1}{4}{Word Mover's Kernel}{equation.3.2}{}}
\newlabel{WME}{{3}{4}{Word Mover's Embedding}{equation.3.3}{}}
\citation{rahimi2007random}
\citation{wu2018d2ke}
\citation{wu2018d2ke}
\citation{kusner2015word,huang2016supervised}
\citation{rubner2000earth}
\newlabel{tb:info of datasets}{{1}{5}{Properties of the datasets\relax }{table.caption.3}{}}
\newlabel{lemma:convergence}{{1}{5}{}{lemma.1}{}}
\newlabel{thm:convergence}{{1}{5}{}{theorem.1}{}}
\newlabel{converge_result}{{1}{5}{}{theorem.1}{}}
\newlabel{sec:Experiments}{{4}{5}{Experiments}{section.4}{}}
\newlabel{sec:Effects of R and D on Random Features}{{4.1}{5}{Effects of $R$ and $D$ on WME}{subsection.4.1}{}}
\citation{kusner2015word,huang2016supervised}
\citation{kusner2015word}
\citation{fan2008liblinear}
\citation{arora2017simple}
\citation{arora2017simple}
\citation{arora2017simple}
\citation{le2014distributed}
\citation{le2014distributed}
\citation{Chen2017efficient}
\newlabel{fig:exptsA_varyingR_twitter}{{2a}{6}{TWITTER\relax }{figure.caption.4}{}}
\newlabel{sub@fig:exptsA_varyingR_twitter}{{a}{6}{TWITTER\relax }{figure.caption.4}{}}
\newlabel{fig:exptsA_varyingR_classic}{{2b}{6}{CLASSIC\relax }{figure.caption.4}{}}
\newlabel{sub@fig:exptsA_varyingR_classic}{{b}{6}{CLASSIC\relax }{figure.caption.4}{}}
\newlabel{fig:exptsA_varyingR}{{2}{6}{Train (Blue) and Test (Red) accuracy when varying $R$ with fixed $D$.\relax }{figure.caption.4}{}}
\newlabel{fig:exptsA_varyingD_twitter}{{3a}{6}{TWITTER\relax }{figure.caption.5}{}}
\newlabel{sub@fig:exptsA_varyingD_twitter}{{a}{6}{TWITTER\relax }{figure.caption.5}{}}
\newlabel{fig:exptsA_varyingD_classic}{{3b}{6}{CLASSIC\relax }{figure.caption.5}{}}
\newlabel{sub@fig:exptsA_varyingD_classic}{{b}{6}{CLASSIC\relax }{figure.caption.5}{}}
\newlabel{fig:exptsA_varyingD}{{3}{6}{Train (Blue) and Test (Red) accuracy when varying $D$ with fixed $R$.\relax }{figure.caption.5}{}}
\newlabel{sec:Comparisons against KNN-WMD in both accuracy and runtime}{{4.2}{6}{Comparison with KNN-WMD}{subsection.4.2}{}}
\newlabel{sec:Comparisons against Word2Vec and Doc2Vec-based document representations}{{4.3}{6}{Comparisons with Word2Vec \& Doc2Vec}{subsection.4.3}{}}
\citation{Chen2017efficient}
\citation{wieting2015ppdb}
\citation{wieting2015towards}
\citation{iyyer2015deep}
\citation{gers2002learning}
\citation{gers2002learning}
\citation{kiros2015skip}
\citation{arora2017simple}
\citation{wieting2015ppdb}
\newlabel{tb:comp_wme_knn_runtime}{{2}{7}{Test accuracy, and total training and testing time (in seconds) of WME against KNN-WMD. Speedups are computed between the best numbers of KNN-WMD+P and these of WME(SR)+P when achieving similar testing accuracy. Bold face highlights the best number for each dataset.\relax }{table.caption.6}{}}
\newlabel{tb:comp_word2vec}{{3}{7}{Testing accuracy of WME against Word2Vec and Doc2Vec-based methods.\relax }{table.caption.7}{}}
\newlabel{sec:Comparisons for performing textual similarity tasks}{{4.4}{7}{Comparisons on textual similarity tasks}{subsection.4.4}{}}
\citation{agirre2012semeval,agirre2013sem,agirre2014semeval,agirre2015semeval}
\citation{xu2015semeval}
\citation{marelli2014semeval}
\citation{socher2011dynamic,kiros2015skip,arora2017simple}
\citation{socher2013recursive}
\citation{li2015hierarchical}
\citation{le2014distributed,Chen2017efficient}
\citation{kiros2015skip}
\citation{kim2016character,gong2018document}
\citation{socher2012semantic,socher2013recursive}
\citation{iyyer2015deep,wieting2015towards}
\citation{kim2014convolutional,kalchbrenner2014convolutional,xu2018graph2seq}
\citation{tai2015improved,liu2015multi}
\newlabel{tb:comp_textual_similarity}{{4}{8}{Pearson's scores of WME against other unsupervised, semi-supervised, and supervised methods on 22 textual similarity tasks. Results are collected from \cite {arora2017simple} except our approach.\relax }{table.caption.8}{}}
\newlabel{sec:Related Work}{{5}{8}{Related Work}{section.5}{}}
\bibstyle{acl_natbib_nourl}
\bibdata{WME_EMNLP18}
\bibcite{agirre2015semeval}{{1}{2015}{{Agirre et~al.}}{{Agirre, Banea, Cardie, Cer, Diab, Gonzalez-Agirre, Guo, Lopez-Gazpio, Maritxalar, Mihalcea et~al.}}}
\bibcite{agirre2014semeval}{{2}{2014}{{Agirre et~al.}}{{Agirre, Banea, Cardie, Cer, Diab, Gonzalez-Agirre, Guo, Mihalcea, Rigau, and Wiebe}}}
\bibcite{agirre2013sem}{{3}{2013}{{Agirre et~al.}}{{Agirre, Cer, Diab, Gonzalez-Agirre, and Guo}}}
\bibcite{agirre2012semeval}{{4}{2012}{{Agirre et~al.}}{{Agirre, Diab, Cer, and Gonzalez-Agirre}}}
\bibcite{altschuler2017near}{{5}{2017}{{Altschuler et~al.}}{{Altschuler, Weed, and Rigollet}}}
\bibcite{arora2016latent}{{6}{2016}{{Arora et~al.}}{{Arora, Li, Liang, Ma, and Risteski}}}
\bibcite{arora2017simple}{{7}{2017}{{Arora et~al.}}{{Arora, Liang, and Ma}}}
\bibcite{bengio2003neural}{{8}{2003}{{Bengio et~al.}}{{Bengio, Ducharme, Vincent, and Jauvin}}}
\bibcite{blei2003latent}{{9}{2003}{{Blei et~al.}}{{Blei, Ng, and Jordan}}}
\bibcite{bourgeois1971extension}{{10}{1971}{{Bourgeois and Lassalle}}{{}}}
\bibcite{buckley1995automatic}{{11}{1995}{{Buckley et~al.}}{{Buckley, Salton, Allan, and Singhal}}}
\bibcite{Chen2017efficient}{{12}{2017}{{Chen}}{{}}}
\bibcite{chen2012marginalized}{{13}{2012}{{Chen et~al.}}{{Chen, Xu, Weinberger, and Sha}}}
\bibcite{deerwester1990indexing}{{14}{1990}{{Deerwester et~al.}}{{Deerwester, Dumais, Furnas, Landauer, and Harshman}}}
\bibcite{fan2008liblinear}{{15}{2008}{{Fan et~al.}}{{Fan, Chang, Hsieh, Wang, and Lin}}}
\bibcite{gers2002learning}{{16}{2002}{{Gers et~al.}}{{Gers, Schraudolph, and Schmidhuber}}}
\bibcite{glorot2011domain}{{17}{2011}{{Glorot et~al.}}{{Glorot, Bordes, and Bengio}}}
\bibcite{gong2018embedding}{{18}{2018{a}}{{Gong et~al.}}{{Gong, Bhat, and Viswanath}}}
\bibcite{gong2017prepositions}{{19}{2017}{{Gong et~al.}}{{Gong, Mu, Bhat, and Viswanath}}}
\bibcite{gong2018document}{{20}{2018{b}}{{Gong et~al.}}{{Gong, Sakakini, Bhat, and Xiong}}}
\bibcite{griffiths2007probabilistic}{{21}{2007}{{Griffiths and Steyvers}}{{}}}
\bibcite{gui2016representative}{{22}{2016}{{Gui et~al.}}{{Gui, Liu, Tao, Sun, and Tan}}}
\bibcite{gui2014estimate}{{23}{2014}{{Gui et~al.}}{{Gui, Sun, Cheng, Ji, and Wu}}}
\bibcite{haasdonk2004learning}{{24}{2004}{{Haasdonk and Bahlmann}}{{}}}
\bibcite{hitchcock1941distribution}{{25}{1941}{{Hitchcock}}{{}}}
\bibcite{huang2016supervised}{{26}{2016}{{Huang et~al.}}{{Huang, Guo, Kusner, Sun, Sha, and Weinberger}}}
\bibcite{iyyer2015deep}{{27}{2015}{{Iyyer et~al.}}{{Iyyer, Manjunatha, Boyd-Graber, and Daum{\'e}~III}}}
\bibcite{kalchbrenner2014convolutional}{{28}{2014}{{Kalchbrenner et~al.}}{{Kalchbrenner, Grefenstette, and Blunsom}}}
\bibcite{kim2014convolutional}{{29}{2014}{{Kim}}{{}}}
\bibcite{kim2016character}{{30}{2016}{{Kim et~al.}}{{Kim, Jernite, Sontag, and Rush}}}
\bibcite{kiros2015skip}{{31}{2015}{{Kiros et~al.}}{{Kiros, Zhu, Salakhutdinov, Zemel, Urtasun, Torralba, and Fidler}}}
\bibcite{kusner2015word}{{32}{2015}{{Kusner et~al.}}{{Kusner, Sun, Kolkin, and Weinberger}}}
\bibcite{le2014distributed}{{33}{2014}{{Le and Mikolov}}{{}}}
\bibcite{li2015hierarchical}{{34}{2015}{{Li et~al.}}{{Li, Luong, and Jurafsky}}}
\bibcite{liu2015multi}{{35}{2015}{{Liu et~al.}}{{Liu, Qiu, Chen, Wu, and Huang}}}
\bibcite{marelli2014semeval}{{36}{2014}{{Marelli et~al.}}{{Marelli, Bentivogli, Baroni, Bernardi, Menini, and Zamparelli}}}
\bibcite{mikolov2013efficient}{{37}{2013{a}}{{Mikolov et~al.}}{{Mikolov, Chen, Corrado, and Dean}}}
\bibcite{mikolov2013exploiting}{{38}{2013{b}}{{Mikolov et~al.}}{{Mikolov, Le, and Sutskever}}}
\bibcite{mikolov2013distributed}{{39}{2013{c}}{{Mikolov et~al.}}{{Mikolov, Sutskever, Chen, Corrado, and Dean}}}
\bibcite{peng2016recurrent}{{40}{2016}{{Peng et~al.}}{{Peng, Feris, Wang, and Metaxas}}}
\bibcite{peng2015piefa}{{41}{2015}{{Peng et~al.}}{{Peng, Zhang, Yang, and Metaxas}}}
\bibcite{pennington2014glove}{{42}{2014}{{Pennington et~al.}}{{Pennington, Socher, and Manning}}}
\bibcite{pham2015learning}{{43}{2015}{{Pham et~al.}}{{Pham, Luong, and Manning}}}
\bibcite{rahimi2007random}{{44}{2007}{{Rahimi and Recht}}{{}}}
\bibcite{robertson1994some}{{45}{1994}{{Robertson and Walker}}{{}}}
\bibcite{robertson1995okapi}{{46}{1995}{{Robertson et~al.}}{{Robertson, Walker, Jones, Hancock-Beaulieu, Gatford et~al.}}}
\bibcite{rubner2000earth}{{47}{2000}{{Rubner et~al.}}{{Rubner, Tomasi, and Guibas}}}
\bibcite{salton1988term}{{48}{1988}{{Salton and Buckley}}{{}}}
\bibcite{socher2011dynamic}{{49}{2011}{{Socher et~al.}}{{Socher, Huang, Pennin, Manning, and Ng}}}
\bibcite{socher2012semantic}{{50}{2012}{{Socher et~al.}}{{Socher, Huval, Manning, and Ng}}}
\bibcite{socher2013recursive}{{51}{2013}{{Socher et~al.}}{{Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts et~al.}}}
\bibcite{tai2015improved}{{52}{2015}{{Tai et~al.}}{{Tai, Socher, and Manning}}}
\bibcite{wang2012baselines}{{53}{2012}{{Wang and Manning}}{{}}}
\bibcite{wieting2015towards}{{54}{2015{a}}{{Wieting et~al.}}{{Wieting, Bansal, Gimpel, and Livescu}}}
\bibcite{wieting2015ppdb}{{55}{2015{b}}{{Wieting et~al.}}{{Wieting, Bansal, Gimpel, Livescu, and Roth}}}
\bibcite{wu2017primme_svds}{{56}{2017}{{Wu et~al.}}{{Wu, Romero, and Stathopoulos}}}
\bibcite{wu2015preconditioned}{{57}{2015}{{Wu and Stathopoulos}}{{}}}
\bibcite{wu2018d2ke}{{58}{2018{a}}{{Wu et~al.}}{{Wu, Yen, Xu, Ravikumar, and Michael}}}
\bibcite{wu2018random}{{59}{2018{b}}{{Wu et~al.}}{{Wu, Yen, Yi, Xu, Lei, and Witbrock}}}
\bibcite{xu2018graph2seq}{{60}{2018}{{Xu et~al.}}{{Xu, Wu, Wang, and Sheinin}}}
\bibcite{xu2015semeval}{{61}{2015}{{Xu et~al.}}{{Xu, Callison-Burch, and Dolan}}}
\bibcite{zhang2018sentence}{{62}{2018}{{Zhang et~al.}}{{Zhang, Liu, and Song}}}
\citation{kusner2015word,huang2016supervised}
\citation{rubner2000earth}
\citation{mikolov2013efficient}
\newlabel{App:Appendix A: Proof of Lemma and Theorem}{{A}{12}{Appendix A: Proof of Lemma \ref {lemma:convergence} and Theorem \ref {thm:convergence}}{appendix.A}{}}
\newlabel{App:Proof of Lemma 1}{{A.1}{12}{Proof of Lemma \ref {lemma:convergence}}{subsection.A.1}{}}
\newlabel{App:Proof of Theorem 1}{{A.2}{12}{Proof of Theorem \ref {thm:convergence}}{subsection.A.2}{}}
\newlabel{tmp1}{{4}{12}{Proof of Theorem \ref {thm:convergence}}{equation.A.4}{}}
\newlabel{tmp2}{{5}{12}{Proof of Theorem \ref {thm:convergence}}{equation.A.5}{}}
\newlabel{tmp3}{{6}{12}{Proof of Theorem \ref {thm:convergence}}{equation.A.6}{}}
\newlabel{App:Appendix B: Additional Experimental Results and Details}{{B}{12}{Appendix B: Additional Experimental Results and Details}{appendix.B}{}}
\newlabel{App:Experimental settings and parameters for WME}{{B.1}{12}{Experimental settings and parameters for WME}{subsection.B.1}{}}
\citation{Chen2017efficient}
\citation{arora2017simple}
\citation{Chen2017efficient}
\citation{arora2017simple}
\citation{arora2017simple}
\citation{arora2017simple}
\citation{buckley1995automatic}
\citation{griffiths2007probabilistic}
\newlabel{App:fig:exptsA_varyingR_bbcsport}{{4a}{13}{BBCSPORT\relax }{figure.caption.10}{}}
\newlabel{sub@App:fig:exptsA_varyingR_bbcsport}{{a}{13}{BBCSPORT\relax }{figure.caption.10}{}}
\newlabel{App:fig:exptsA_varyingR_twitter}{{4b}{13}{TWITTER\relax }{figure.caption.10}{}}
\newlabel{sub@App:fig:exptsA_varyingR_twitter}{{b}{13}{TWITTER\relax }{figure.caption.10}{}}
\newlabel{App:fig:exptsA_varyingR_recipe2}{{4c}{13}{RECIPE\relax }{figure.caption.10}{}}
\newlabel{sub@App:fig:exptsA_varyingR_recipe2}{{c}{13}{RECIPE\relax }{figure.caption.10}{}}
\newlabel{App:fig:exptsA_varyingR_ohsumed}{{4d}{13}{OHSUMED\relax }{figure.caption.10}{}}
\newlabel{sub@App:fig:exptsA_varyingR_ohsumed}{{d}{13}{OHSUMED\relax }{figure.caption.10}{}}
\newlabel{App:fig:exptsA_varyingR_classic}{{4e}{13}{CLASSIC\relax }{figure.caption.10}{}}
\newlabel{sub@App:fig:exptsA_varyingR_classic}{{e}{13}{CLASSIC\relax }{figure.caption.10}{}}
\newlabel{App:fig:exptsA_varyingR_r8}{{4f}{13}{REUTERS\relax }{figure.caption.10}{}}
\newlabel{sub@App:fig:exptsA_varyingR_r8}{{f}{13}{REUTERS\relax }{figure.caption.10}{}}
\newlabel{App:fig:exptsA_varyingR_amazon}{{4g}{13}{AMAZON\relax }{figure.caption.10}{}}
\newlabel{sub@App:fig:exptsA_varyingR_amazon}{{g}{13}{AMAZON\relax }{figure.caption.10}{}}
\newlabel{App:fig:exptsA_varyingR_20ng2_500}{{4h}{13}{20NEWS\relax }{figure.caption.10}{}}
\newlabel{sub@App:fig:exptsA_varyingR_20ng2_500}{{h}{13}{20NEWS\relax }{figure.caption.10}{}}
\newlabel{App:fig:exptsA_varyingR}{{4}{13}{Train (Blue) and test (Red) accuracy when varying $R$ with fixed $D$.\relax }{figure.caption.10}{}}
\newlabel{App:fig:exptsA_varyingD_bbcsport}{{5a}{13}{BBCSPORT\relax }{figure.caption.11}{}}
\newlabel{sub@App:fig:exptsA_varyingD_bbcsport}{{a}{13}{BBCSPORT\relax }{figure.caption.11}{}}
\newlabel{App:fig:exptsA_varyingD_twitter}{{5b}{13}{TWITTER\relax }{figure.caption.11}{}}
\newlabel{sub@App:fig:exptsA_varyingD_twitter}{{b}{13}{TWITTER\relax }{figure.caption.11}{}}
\newlabel{App:fig:exptsA_varyingD_recipe2}{{5c}{13}{RECIPE\relax }{figure.caption.11}{}}
\newlabel{sub@App:fig:exptsA_varyingD_recipe2}{{c}{13}{RECIPE\relax }{figure.caption.11}{}}
\newlabel{App:fig:exptsA_varyingD_ohsumed}{{5d}{13}{OHSUMED\relax }{figure.caption.11}{}}
\newlabel{sub@App:fig:exptsA_varyingD_ohsumed}{{d}{13}{OHSUMED\relax }{figure.caption.11}{}}
\newlabel{App:fig:exptsA_varyingD_classic}{{5e}{13}{CLASSIC\relax }{figure.caption.11}{}}
\newlabel{sub@App:fig:exptsA_varyingD_classic}{{e}{13}{CLASSIC\relax }{figure.caption.11}{}}
\newlabel{App:fig:exptsA_varyingD_r8}{{5f}{13}{REUTERS\relax }{figure.caption.11}{}}
\newlabel{sub@App:fig:exptsA_varyingD_r8}{{f}{13}{REUTERS\relax }{figure.caption.11}{}}
\newlabel{App:fig:exptsA_varyingD_amazon}{{5g}{13}{AMAZON\relax }{figure.caption.11}{}}
\newlabel{sub@App:fig:exptsA_varyingD_amazon}{{g}{13}{AMAZON\relax }{figure.caption.11}{}}
\newlabel{App:fig:exptsA_varyingD_20ng2_500}{{5h}{13}{20NEWS\relax }{figure.caption.11}{}}
\newlabel{sub@App:fig:exptsA_varyingD_20ng2_500}{{h}{13}{20NEWS\relax }{figure.caption.11}{}}
\newlabel{App:fig:exptsA_varyingD}{{5}{13}{Train (Blue) and test (Red) accuracy when varying $D$ with fixed $R$.\relax }{figure.caption.11}{}}
\newlabel{App:More results about effects of $R$ and $D$ on random documents}{{B.2}{13}{More results about effects of $R$ and $D$ on random documents}{subsection.B.2}{}}
\newlabel{App:More results on Comparisons against distance-based methods}{{B.3}{13}{More results on Comparisons against distance-based methods}{subsection.B.3}{}}
\citation{salton1988term}
\citation{robertson1994some}
\citation{robertson1995okapi}
\citation{deerwester1990indexing}
\citation{wu2015preconditioned,wu2017primme_svds}
\citation{blei2003latent}
\citation{chen2012marginalized}
\citation{glorot2011domain}
\citation{Chen2017efficient}
\newlabel{tb:comp_knn}{{5}{14}{Testing accuracy comparing WME against KNN-based methods\relax }{table.caption.12}{}}
\newlabel{App:tb:comp_word2vec}{{6}{14}{Testing accuracy of WME against Word2Vec and Doc2Vec-based methods.\relax }{table.caption.13}{}}
\newlabel{App:More Results on Comparisons against Word2Vec and Doc2Vec-based document representations}{{B.4}{14}{More results on comparisons against Word2Vec and Doc2Vec-based document representations}{subsection.B.4}{}}
\newlabel{tb:comp_imdb}{{7}{15}{Testing accuracy of WME against other document representations on Imdb dataset (50K). Results are collected from \cite {Chen2017efficient} and \cite {arora2017simple}.\relax }{table.caption.14}{}}
\newlabel{tb:comp_textual_similarity_full}{{8}{15}{Pearson's scores of WME against other unsupervised and supervised methods on 22 textual similarity tasks. Results are collected from \cite {arora2017simple} except our approach.\relax }{table.caption.15}{}}
\newlabel{App:More results on comparisons for textual similarity tasks}{{B.5}{15}{More results on comparisons for textual similarity tasks}{subsection.B.5}{}}
